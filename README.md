# 2018
1- H. F. Langroudi et. al, [Deep learning inference on embedded devices: Fixed-point vs posit](https://ieeexplore.ieee.org/abstract/document/8524018), EMC2 workshop co-located with ASPLOS, March 25. 

2- M. Cococcioni et. al., [Exploiting Posit Arithmetic for Deep Neural Networks in Autonomous Driving Applications](https://ieeexplore.ieee.org/abstract/document/8493233), EETA, July 9.

3- Erwin de Haan et. al, [Towards Accelerating Deep Neural Network Training on FPGAs: Facilitating the Use of Variable Precision](https://scholar.google.com/scholar?cluster=8230763715104912639&hl=en&as_sdt=5,44&as_ylo=2017), IPSJ-HPC Technical Report, July 31. 

4- H. F. Langroudi et. al., [PositNN: Tapered Precision Deep Learning Inference for the Edge](https://openreview.net/forum?id=HJGh64VFo7), Open Review, Oct. 20.

5- Jeff Johnson, [Rethinking floating point for deep learning](https://arxiv.org/abs/1811.01721), NeurIPS Systems for ML Workshop, Nov 1.

6- Zishen Wan, [Study of Posit Numeric in Speech Recognition Neural Inference](https://scholar.harvard.edu/files/zishenwan/files/cs247r_project.pdf), CS247r Harvard Tech Rep, Dec 15. 

# 2019
7- Carmichael et. al., [Performance-Efficiency Trade-Off of Low-Precision Numerical Formats in Deep Neural Networks](https://dl.acm.org/doi/abs/10.1145/3316279.3316282), CoNGA, March 13. 

8- Carmichael et. al., [Deep Positron: A Deep Neural Network Using the Posit Number System](https://ieeexplore.ieee.org/abstract/document/8715262), DATE, March 25.

9- Raúl M. Montero et. al, [TEMPLATE-BASED POSIT MULTIPLICATION FOR TRAINING AND INFERRING IN NEURAL NETWORKS](https://arxiv.org/pdf/1907.04091.pdf), arXiv, July 9. 

10- H. F. Langroudi et. al, [PositNN Framework: Tapered Precision Deep Learning Inference for the Edge](https://ieeexplore.ieee.org/abstract/document/8853677), SC, July 30. 

11- H. F. Langroudi et. al, [Deep Learning Training on the Edge with Low-Precision Posits](https://arxiv.org/abs/1907.13216), arXiv, July 30. 

12- H. F. Langroudi et. al, [Cheetah: Mixed Low-Precision Hardware & Software Co-Design Framework for DNNs on the Edge](https://arxiv.org/abs/1908.02386), arXiv, Aug 6.

13- Jinming Lu et. al., [Training Deep Neural Networks Using Posit Number System](https://arxiv.org/abs/1909.03831), arXiv, Sept. 6. 

14- M. Cococcioni et. al., [Novel arithmetics to accelerate machine learning classifiers in autonomous driving applications](https://ieeexplore.ieee.org/abstract/document/8965031), ICECS, Nov 27. 

# 2020
15- Andre Guntoro et. al., [Next Generation Arithmetic for Edge Computing](https://ieeexplore.ieee.org/document/9116196), DATE, March 9.

16- M. Cococcioni et. al., [Fast Approximations of Activation Functions in Deep Neural Networks when using Posit Arithmetic](https://www.mdpi.com/1424-8220/20/5/1515), Sensors, March 10.

17- Jinming Lu et. al., [Evaluations on Deep Neural Networks Training Using Posit Number System](https://ieeexplore.ieee.org/document/9066876), IEEE TC, April 14.

18- Lukas Sommer et. al., [Comparison of Arithmetic Number Formats for Inference in Sum-Product Networks on FPGAs](https://ieeexplore.ieee.org/document/9114810), FCCM, May 3. 

19- Raúl M. Montero et. al, [Deep PeNSieve: A deep learning framework based on the posit number system](https://www.sciencedirect.com/science/article/pii/S105120042030107X), DSP, May 7.

20- M. Cococcioni et. al., [Fast deep neural networks for image processing using posits and ARM scalable vector extension](https://link.springer.com/article/10.1007/s11554-020-00984-x), JRTIP, May 18.

21- Nano Nevas et. al., [Reconfigurable Stream-based Tensor Unit with Variable-Precision Posit Arithmetic](https://ieeexplore.ieee.org/document/9153231), ASAP, July 6. 

22- H. F. Langroudi et. al, [Adaptive Posit: Parameter aware numerical format for deep learning inference
on the edge](https://ieeexplore.ieee.org/abstract/document/9151086), CVPRW, July 30.

23- M. Cococcioni et. al., [A Novel Posit-based Fast Approximation of ELU Activation Function for Deep Neural Networks](https://ieeexplore.ieee.org/abstract/document/9239674), SMARTCOMP, Sept. 14. 

24- Suresh Nambi et. al., [ExPAN(N)D: Exploring Posits for Efficient Artificial Neural Network Design in FPGA-based Systems](https://ieeexplore.ieee.org/abstract/document/9492075), IEEE Access, Oct. 24.

25-  M. Cococcioni et. al., [Novel Arithmetics in Deep Neural Networks Signal Processing for Autonomous Driving: Challenges and Opportunities](https://ieeexplore.ieee.org/abstract/document/9307291), IEEE SPM, Dec. 24. 

# 2021
26- Ihsen Alouani et. al., [An Investigation on Inherent Robustness of Posit Data Representation](https://ieeexplore.ieee.org/document/9407364), VLSID, Jan 5.

27- Nhut-Minh Ho et. al., [Posit Arithmetic for the Training and Deployment of Generative Adversarial Networks](https://ieeexplore.ieee.org/abstract/document/9473933), DATE, Feb 1.

28- Nimish Shah et. al., [PIU: A 248GOPS/W Stream-Based Processor for Irregular Probabilistic Inference Networks Using Precision-Scalable Posit Arithmetic in 28nm](https://ieeexplore.ieee.org/abstract/document/9366061), ISSCC, Feb 13.

29- Raúl M. Montero et. al., [PLAM: a Posit Logarithm-Approximate Multiplier](https://arxiv.org/abs/2102.09262v2), TETC, Feb 18. 

30- Varun Gohil et.al, [Fixed-Posit: A Floating-Point Representation for Error-Resilient Applications](https://ieeexplore.ieee.org/abstract/document/9399648), TCAS-II, April 10.

31- Aleksander YU. Romanov. et. al., [Analysis of Posit and Bfloat Arithmetic of Real Numbers for Machine Learning](https://ieeexplore.ieee.org/document/9446981), IEEE Access, June 4.

32- Gonc¸alo Raposo et. al, [POSITNN: TRAINING DEEP NEURAL NETWORKS WITH MIXED LOW-PRECISION POSIT](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9413919), ICASSP, Jun 6

33- Yang Wang et. al, [LPE: Logarithm Posit Processing Element for Energy-Efficient Edge-Device Training](https://ieeexplore.ieee.org/abstract/document/9458421/authors#authors), AICAS, Jun 9. 

34- H. F. Langroudi et. al, [ALPS: Adaptive Quantization of Deep Neural Networks With GeneraLized PositS](https://ieeexplore.ieee.org/document/9522706), CVPRW, June 19.

35- Stefan Dan Ciocirlan et. al, [The Accuracy and Efficiency of Posit Arithmetic](https://arxiv.org/abs/2104.04763), arXiv, Sept. 16. 

36- M. Cococcioni et. al., [A Lightweight Posit Processing Unit for RISC-V Processors in Deep Neural Network Applications](https://ieeexplore.ieee.org/document/9583876), TETC, Oct. 21. 

37- S. Walia et al. , [Fast and low-power quantized fixed posit high-accuracy DNN implementation.](https://ieeexplore.ieee.org/abstract/document/9764927), TVLSI, Dec, 10.

# 2022
38- M. Cococcioni et. al., [ Small reals representations for Deep Learning at the edge: a comparison ](https://link.springer.com/chapter/10.1007/978-3-031-09779-9_8),
CoNGA'22, March 2.

39- H. F. Langroudi et. al., [ACTION: Automated Hardware-Software Codesign Framework for Low-precision Numerical Format SelecTION in TinyML](https://link.springer.com/chapter/10.1007/978-3-031-09779-9_4), CoNGA'22, March 2.

40- Nhut-Minh Ho et. al., [Qtorch+: Next Generation Arithmetic for Pytorch Machine Learning](https://link.springer.com/chapter/10.1007/978-3-031-09779-9_3), CoNGA'22, March 2.

41- O. Desrentes et. al., [A Posit8 Decompression Operator for Deep Neural Network Inference](https://link.springer.com/chapter/10.1007/978-3-031-09779-9_2),  CoNGA'22, March 2.

42- M. Cococcioni et. al., [Experimental Results of Vectorized Posit-Based DNNs on a Real ARM SVE High Performance Computing Machine](https://link.springer.com/chapter/10.1007/978-3-030-95498-7_9), APPLEPIES, March 2.

42- M. Zolfagharinejad et al. , [Posit Process Element for Using in Energy-Efficient DNN Accelerators](https://ieeexplore.ieee.org/document/9761139), TVLSI, April, 21. 

43-Y. Nakahara et al. , [A Posit Based Multiply-accumulate Unit with Small Quire Size for Deep Neural Networks](https://www.jstage.jst.go.jp/article/ipsjtsldm/15/0/15_16/_article/-char/en), IPSJ-LSI, June 14. 
